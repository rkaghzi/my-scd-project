
==> Audit <==
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start      | --driver=docker                | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 02:44 PKT | 14 May 25 02:50 PKT |
| start      |                                | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:03 PKT | 14 May 25 03:05 PKT |
| docker-env | minikube docker-env            | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:09 PKT | 14 May 25 03:09 PKT |
| docker-env |                                | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:10 PKT | 14 May 25 03:10 PKT |
| docker-env |                                | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:11 PKT | 14 May 25 03:11 PKT |
| docker-env | minikube docker-env --shell    | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:13 PKT | 14 May 25 03:13 PKT |
|            | powershell                     |          |                       |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:15 PKT | 14 May 25 03:15 PKT |
|            | powershell                     |          |                       |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:33 PKT | 14 May 25 03:33 PKT |
| docker-env | minikube docker-env            | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:34 PKT | 14 May 25 03:34 PKT |
| service    | my-app-service                 | minikube | DESKTOP-LK0M0O3\rhuss | v1.35.0 | 14 May 25 03:43 PKT |                     |
|------------|--------------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/14 03:03:41
Running on machine: DESKTOP-LK0M0O3
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0514 03:03:41.023199   14292 out.go:345] Setting OutFile to fd 84 ...
I0514 03:03:41.023199   14292 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0514 03:03:41.023199   14292 out.go:358] Setting ErrFile to fd 88...
I0514 03:03:41.023199   14292 out.go:392] TERM=,COLORTERM=, which probably does not support color
W0514 03:03:41.044548   14292 root.go:314] Error reading config file at C:\Users\rhuss\.minikube\config\config.json: open C:\Users\rhuss\.minikube\config\config.json: The system cannot find the file specified.
I0514 03:03:41.055728   14292 out.go:352] Setting JSON to false
I0514 03:03:41.060338   14292 start.go:129] hostinfo: {"hostname":"DESKTOP-LK0M0O3","uptime":134785,"bootTime":1747039035,"procs":248,"os":"windows","platform":"Microsoft Windows 10 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5796 Build 19045.5796","kernelVersion":"10.0.19045.5796 Build 19045.5796","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"5ea740d8-032d-4848-99cf-ea6288ed1209"}
W0514 03:03:41.060338   14292 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0514 03:03:41.062024   14292 out.go:177] * minikube v1.35.0 on Microsoft Windows 10 Pro 10.0.19045.5796 Build 19045.5796
I0514 03:03:41.064273   14292 notify.go:220] Checking for updates...
I0514 03:03:41.066135   14292 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0514 03:03:41.066644   14292 driver.go:394] Setting default libvirt URI to qemu:///system
I0514 03:03:41.223734   14292 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0514 03:03:41.241279   14292 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0514 03:03:41.895790   14292 info.go:266] docker info: {ID:b89f2f01-c9c9-49f9-8261-be240cdba1b6 Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:7 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:true NGoroutines:102 SystemTime:2025-05-13 22:03:41.854418628 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8286752768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0514 03:03:41.898074   14292 out.go:177] * Using the docker driver based on existing profile
I0514 03:03:41.898625   14292 start.go:297] selected driver: docker
I0514 03:03:41.898625   14292 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rhuss:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0514 03:03:41.899180   14292 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0514 03:03:41.929601   14292 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0514 03:03:42.754457   14292 info.go:266] docker info: {ID:b89f2f01-c9c9-49f9-8261-be240cdba1b6 Containers:4 ContainersRunning:1 ContainersPaused:0 ContainersStopped:3 Images:7 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:84 OomKillDisable:true NGoroutines:102 SystemTime:2025-05-13 22:03:42.713754253 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8286752768 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0514 03:03:42.903147   14292 cni.go:84] Creating CNI manager for ""
I0514 03:03:42.903147   14292 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0514 03:03:42.903676   14292 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rhuss:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0514 03:03:42.904894   14292 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0514 03:03:42.907211   14292 cache.go:121] Beginning downloading kic base image for docker with docker
I0514 03:03:42.908502   14292 out.go:177] * Pulling base image v0.0.46 ...
I0514 03:03:42.910241   14292 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0514 03:03:42.910241   14292 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0514 03:03:42.910797   14292 preload.go:146] Found local preload: C:\Users\rhuss\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0514 03:03:42.910797   14292 cache.go:56] Caching tarball of preloaded images
I0514 03:03:42.911531   14292 preload.go:172] Found C:\Users\rhuss\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0514 03:03:42.911531   14292 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0514 03:03:42.912072   14292 profile.go:143] Saving config to C:\Users\rhuss\.minikube\profiles\minikube\config.json ...
I0514 03:03:43.255793   14292 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0514 03:03:43.256354   14292 localpath.go:146] windows sanitize: C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0514 03:03:43.256894   14292 localpath.go:146] windows sanitize: C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0514 03:03:43.256894   14292 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0514 03:03:43.256894   14292 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0514 03:03:43.256894   14292 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0514 03:03:43.257444   14292 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0514 03:03:43.257444   14292 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0514 03:03:43.257535   14292 localpath.go:146] windows sanitize: C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\rhuss\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0514 03:04:51.120385   14292 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0514 03:04:51.120385   14292 cache.go:227] Successfully downloaded all kic artifacts
I0514 03:04:51.121455   14292 start.go:360] acquireMachinesLock for minikube: {Name:mk5398f7e733d223ea897635149b593d9ceae5e0 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0514 03:04:51.121743   14292 start.go:364] duration metric: took 217.1Âµs to acquireMachinesLock for "minikube"
I0514 03:04:51.121743   14292 start.go:96] Skipping create...Using existing machine configuration
I0514 03:04:51.121743   14292 fix.go:54] fixHost starting: 
I0514 03:04:51.173030   14292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0514 03:04:51.309187   14292 fix.go:112] recreateIfNeeded on minikube: state=Running err=<nil>
W0514 03:04:51.309187   14292 fix.go:138] unexpected machine state, will restart: <nil>
I0514 03:04:51.311586   14292 out.go:177] * Updating the running docker "minikube" container ...
I0514 03:04:51.313036   14292 machine.go:93] provisionDockerMachine start ...
I0514 03:04:51.336432   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:51.480371   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:51.481744   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:51.481744   14292 main.go:141] libmachine: About to run SSH command:
hostname
I0514 03:04:51.680129   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0514 03:04:51.680238   14292 ubuntu.go:169] provisioning hostname "minikube"
I0514 03:04:51.695796   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:51.790174   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:51.790880   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:51.790880   14292 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0514 03:04:52.031208   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0514 03:04:52.052775   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:52.177174   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:52.178087   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:52.178087   14292 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0514 03:04:52.412333   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0514 03:04:52.412462   14292 ubuntu.go:175] set auth options {CertDir:C:\Users\rhuss\.minikube CaCertPath:C:\Users\rhuss\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\rhuss\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\rhuss\.minikube\machines\server.pem ServerKeyPath:C:\Users\rhuss\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\rhuss\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\rhuss\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\rhuss\.minikube}
I0514 03:04:52.412462   14292 ubuntu.go:177] setting up certificates
I0514 03:04:52.412462   14292 provision.go:84] configureAuth start
I0514 03:04:52.433959   14292 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0514 03:04:52.527615   14292 provision.go:143] copyHostCerts
I0514 03:04:52.528152   14292 exec_runner.go:144] found C:\Users\rhuss\.minikube/ca.pem, removing ...
I0514 03:04:52.528152   14292 exec_runner.go:203] rm: C:\Users\rhuss\.minikube\ca.pem
I0514 03:04:52.528705   14292 exec_runner.go:151] cp: C:\Users\rhuss\.minikube\certs\ca.pem --> C:\Users\rhuss\.minikube/ca.pem (1074 bytes)
I0514 03:04:52.530431   14292 exec_runner.go:144] found C:\Users\rhuss\.minikube/cert.pem, removing ...
I0514 03:04:52.530431   14292 exec_runner.go:203] rm: C:\Users\rhuss\.minikube\cert.pem
I0514 03:04:52.531445   14292 exec_runner.go:151] cp: C:\Users\rhuss\.minikube\certs\cert.pem --> C:\Users\rhuss\.minikube/cert.pem (1119 bytes)
I0514 03:04:52.531445   14292 exec_runner.go:144] found C:\Users\rhuss\.minikube/key.pem, removing ...
I0514 03:04:52.531445   14292 exec_runner.go:203] rm: C:\Users\rhuss\.minikube\key.pem
I0514 03:04:52.533371   14292 exec_runner.go:151] cp: C:\Users\rhuss\.minikube\certs\key.pem --> C:\Users\rhuss\.minikube/key.pem (1679 bytes)
I0514 03:04:52.533917   14292 provision.go:117] generating server cert: C:\Users\rhuss\.minikube\machines\server.pem ca-key=C:\Users\rhuss\.minikube\certs\ca.pem private-key=C:\Users\rhuss\.minikube\certs\ca-key.pem org=rhuss.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0514 03:04:52.858361   14292 provision.go:177] copyRemoteCerts
I0514 03:04:52.903031   14292 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0514 03:04:52.917523   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:53.012473   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:04:53.170985   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0514 03:04:53.228679   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0514 03:04:53.300733   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0514 03:04:53.377448   14292 provision.go:87] duration metric: took 964.8785ms to configureAuth
I0514 03:04:53.377448   14292 ubuntu.go:193] setting minikube options for container-runtime
I0514 03:04:53.379882   14292 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0514 03:04:53.397528   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:53.487652   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:53.488192   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:53.488192   14292 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0514 03:04:53.708171   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0514 03:04:53.708171   14292 ubuntu.go:71] root file system type: overlay
I0514 03:04:53.708725   14292 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0514 03:04:53.740621   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:53.878918   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:53.879483   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:53.880042   14292 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0514 03:04:54.082365   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0514 03:04:54.107401   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:54.201691   14292 main.go:141] libmachine: Using SSH client type: native
I0514 03:04:54.202227   14292 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1055360] 0x1057ea0 <nil>  [] 0s} 127.0.0.1 59274 <nil> <nil>}
I0514 03:04:54.202227   14292 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0514 03:04:54.390005   14292 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0514 03:04:54.390005   14292 machine.go:96] duration metric: took 3.0769693s to provisionDockerMachine
I0514 03:04:54.390005   14292 start.go:293] postStartSetup for "minikube" (driver="docker")
I0514 03:04:54.390005   14292 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0514 03:04:54.416901   14292 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0514 03:04:54.431020   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:54.509696   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:04:54.674463   14292 ssh_runner.go:195] Run: cat /etc/os-release
I0514 03:04:54.684751   14292 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0514 03:04:54.684751   14292 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0514 03:04:54.684751   14292 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0514 03:04:54.684751   14292 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0514 03:04:54.684751   14292 filesync.go:126] Scanning C:\Users\rhuss\.minikube\addons for local assets ...
I0514 03:04:54.685819   14292 filesync.go:126] Scanning C:\Users\rhuss\.minikube\files for local assets ...
I0514 03:04:54.685819   14292 start.go:296] duration metric: took 295.8137ms for postStartSetup
I0514 03:04:54.708967   14292 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0514 03:04:54.722022   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:54.795072   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:04:54.944980   14292 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0514 03:04:54.957101   14292 fix.go:56] duration metric: took 3.8353578s for fixHost
I0514 03:04:54.957101   14292 start.go:83] releasing machines lock for "minikube", held for 3.8353578s
I0514 03:04:54.971576   14292 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0514 03:04:55.055109   14292 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0514 03:04:55.077048   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:55.085808   14292 ssh_runner.go:195] Run: cat /version.json
I0514 03:04:55.108703   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:04:55.189520   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:04:55.220352   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
W0514 03:04:55.335336   14292 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0514 03:04:55.399466   14292 ssh_runner.go:195] Run: systemctl --version
I0514 03:04:55.450900   14292 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0514 03:04:55.484641   14292 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0514 03:04:55.504602   14292 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0514 03:04:55.528728   14292 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0514 03:04:55.548164   14292 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0514 03:04:55.548164   14292 start.go:495] detecting cgroup driver to use...
I0514 03:04:55.548679   14292 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0514 03:04:55.548747   14292 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0514 03:04:55.607050   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0514 03:04:55.669210   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0514 03:04:55.701708   14292 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0514 03:04:55.728504   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0514 03:04:55.795444   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0514 03:04:55.874808   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0514 03:04:55.924634   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0514 03:04:55.971236   14292 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0514 03:04:56.016211   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0514 03:04:56.061249   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0514 03:04:56.122579   14292 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0514 03:04:56.171539   14292 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0514 03:04:56.217922   14292 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0514 03:04:56.266210   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0514 03:04:56.464808   14292 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0514 03:04:56.464808   14292 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0514 03:04:56.519038   14292 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0514 03:05:07.312331   14292 ssh_runner.go:235] Completed: sudo systemctl restart containerd: (10.7932939s)
I0514 03:05:07.312331   14292 start.go:495] detecting cgroup driver to use...
I0514 03:05:07.312476   14292 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0514 03:05:07.365404   14292 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0514 03:05:07.400453   14292 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0514 03:05:07.433405   14292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0514 03:05:07.467459   14292 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0514 03:05:07.535517   14292 ssh_runner.go:195] Run: which cri-dockerd
I0514 03:05:07.588097   14292 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0514 03:05:07.620969   14292 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0514 03:05:07.691801   14292 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0514 03:05:07.927176   14292 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0514 03:05:08.117686   14292 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0514 03:05:08.118260   14292 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0514 03:05:08.201082   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0514 03:05:08.426330   14292 ssh_runner.go:195] Run: sudo systemctl restart docker
I0514 03:05:13.587513   14292 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.1611825s)
I0514 03:05:13.651805   14292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0514 03:05:13.744563   14292 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0514 03:05:13.898762   14292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0514 03:05:14.010666   14292 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0514 03:05:14.251133   14292 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0514 03:05:14.534107   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0514 03:05:14.818769   14292 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0514 03:05:14.871295   14292 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0514 03:05:14.922885   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0514 03:05:15.198999   14292 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0514 03:05:15.340165   14292 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0514 03:05:15.371735   14292 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0514 03:05:15.384148   14292 start.go:563] Will wait 60s for crictl version
I0514 03:05:15.417520   14292 ssh_runner.go:195] Run: which crictl
I0514 03:05:15.454575   14292 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0514 03:05:15.518889   14292 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0514 03:05:15.532204   14292 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0514 03:05:15.592595   14292 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0514 03:05:15.638048   14292 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0514 03:05:15.652387   14292 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0514 03:05:15.899403   14292 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0514 03:05:15.944149   14292 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0514 03:05:15.988315   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0514 03:05:16.115413   14292 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rhuss:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0514 03:05:16.115738   14292 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0514 03:05:16.138532   14292 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0514 03:05:16.197045   14292 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0514 03:05:16.197045   14292 docker.go:619] Images already preloaded, skipping extraction
I0514 03:05:16.218150   14292 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0514 03:05:16.274364   14292 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0514 03:05:16.274894   14292 cache_images.go:84] Images are preloaded, skipping loading
I0514 03:05:16.274943   14292 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0514 03:05:16.274943   14292 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0514 03:05:16.290766   14292 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0514 03:05:16.407829   14292 cni.go:84] Creating CNI manager for ""
I0514 03:05:16.407829   14292 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0514 03:05:16.407829   14292 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0514 03:05:16.407829   14292 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0514 03:05:16.408345   14292 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0514 03:05:16.437493   14292 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0514 03:05:16.465300   14292 binaries.go:44] Found k8s binaries, skipping transfer
I0514 03:05:16.527071   14292 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0514 03:05:16.555819   14292 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0514 03:05:16.628850   14292 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0514 03:05:16.722829   14292 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0514 03:05:16.927452   14292 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0514 03:05:17.047131   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0514 03:05:17.953858   14292 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0514 03:05:18.114007   14292 certs.go:68] Setting up C:\Users\rhuss\.minikube\profiles\minikube for IP: 192.168.49.2
I0514 03:05:18.114007   14292 certs.go:194] generating shared ca certs ...
I0514 03:05:18.114007   14292 certs.go:226] acquiring lock for ca certs: {Name:mk6baa00e950e131efa70e26183fa3fc3c9d4db3 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0514 03:05:18.115194   14292 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\rhuss\.minikube\ca.key
I0514 03:05:18.115748   14292 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\rhuss\.minikube\proxy-client-ca.key
I0514 03:05:18.115748   14292 certs.go:256] generating profile certs ...
I0514 03:05:18.117440   14292 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\rhuss\.minikube\profiles\minikube\client.key
I0514 03:05:18.117507   14292 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\rhuss\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0514 03:05:18.118345   14292 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\rhuss\.minikube\profiles\minikube\proxy-client.key
I0514 03:05:18.120095   14292 certs.go:484] found cert: C:\Users\rhuss\.minikube\certs\ca-key.pem (1679 bytes)
I0514 03:05:18.120974   14292 certs.go:484] found cert: C:\Users\rhuss\.minikube\certs\ca.pem (1074 bytes)
I0514 03:05:18.121365   14292 certs.go:484] found cert: C:\Users\rhuss\.minikube\certs\cert.pem (1119 bytes)
I0514 03:05:18.121365   14292 certs.go:484] found cert: C:\Users\rhuss\.minikube\certs\key.pem (1679 bytes)
I0514 03:05:18.123775   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0514 03:05:18.420790   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0514 03:05:18.794546   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0514 03:05:18.981278   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0514 03:05:19.312340   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0514 03:05:19.585577   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0514 03:05:19.911106   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0514 03:05:20.193614   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0514 03:05:20.690958   14292 ssh_runner.go:362] scp C:\Users\rhuss\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0514 03:05:21.074366   14292 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0514 03:05:21.229498   14292 ssh_runner.go:195] Run: openssl version
I0514 03:05:21.334514   14292 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0514 03:05:21.736891   14292 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0514 03:05:22.001738   14292 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 13 21:50 /usr/share/ca-certificates/minikubeCA.pem
I0514 03:05:22.038037   14292 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0514 03:05:22.231042   14292 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0514 03:05:22.436991   14292 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0514 03:05:22.525429   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0514 03:05:22.721564   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0514 03:05:22.941980   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0514 03:05:23.051798   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0514 03:05:23.138639   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0514 03:05:23.335806   14292 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0514 03:05:23.497210   14292 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\rhuss:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0514 03:05:23.522504   14292 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0514 03:05:24.138540   14292 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0514 03:05:24.307814   14292 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0514 03:05:24.307814   14292 kubeadm.go:593] restartPrimaryControlPlane start ...
I0514 03:05:24.341933   14292 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0514 03:05:24.400679   14292 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0514 03:05:24.425944   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0514 03:05:24.541012   14292 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:59278"
I0514 03:05:24.566190   14292 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0514 03:05:24.675984   14292 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0514 03:05:24.675984   14292 kubeadm.go:597] duration metric: took 368.1697ms to restartPrimaryControlPlane
I0514 03:05:24.675984   14292 kubeadm.go:394] duration metric: took 1.1787742s to StartCluster
I0514 03:05:24.676578   14292 settings.go:142] acquiring lock: {Name:mkafba3de838bded9ac76128943c1b40b6c290b4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0514 03:05:24.676578   14292 settings.go:150] Updating kubeconfig:  C:\Users\rhuss\.kube\config
I0514 03:05:24.678844   14292 lock.go:35] WriteFile acquiring C:\Users\rhuss\.kube\config: {Name:mk36289e1994c4c61d975415992c9221c313aec1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0514 03:05:24.680699   14292 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0514 03:05:24.681513   14292 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0514 03:05:24.681513   14292 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0514 03:05:24.681513   14292 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0514 03:05:24.681513   14292 addons.go:247] addon storage-provisioner should already be in state true
I0514 03:05:24.682051   14292 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0514 03:05:24.682051   14292 host.go:66] Checking if "minikube" exists ...
I0514 03:05:24.682051   14292 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0514 03:05:24.682051   14292 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0514 03:05:24.699024   14292 out.go:177] * Verifying Kubernetes components...
I0514 03:05:24.740205   14292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0514 03:05:24.743666   14292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0514 03:05:24.763898   14292 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0514 03:05:24.933922   14292 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0514 03:05:24.933922   14292 addons.go:247] addon default-storageclass should already be in state true
I0514 03:05:24.934470   14292 host.go:66] Checking if "minikube" exists ...
I0514 03:05:24.988605   14292 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0514 03:05:24.990191   14292 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0514 03:05:24.990191   14292 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0514 03:05:25.007133   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:05:25.007133   14292 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0514 03:05:25.179180   14292 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0514 03:05:25.179180   14292 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0514 03:05:25.201836   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0514 03:05:25.326344   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:05:25.452729   14292 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:59274 SSHKeyPath:C:\Users\rhuss\.minikube\machines\minikube\id_rsa Username:docker}
I0514 03:05:26.194792   14292 ssh_runner.go:235] Completed: sudo systemctl daemon-reload: (1.430894s)
I0514 03:05:26.228421   14292 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0514 03:05:26.423076   14292 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0514 03:05:26.442837   14292 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0514 03:05:26.450538   14292 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0514 03:05:26.533307   14292 api_server.go:52] waiting for apiserver process to appear ...
I0514 03:05:26.555382   14292 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0514 03:05:30.884228   14292 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (4.43369s)
I0514 03:05:33.441007   14292 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.9976186s)
I0514 03:05:33.441007   14292 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (6.8856253s)
I0514 03:05:33.441007   14292 api_server.go:72] duration metric: took 8.7603086s to wait for apiserver process to appear ...
I0514 03:05:33.441007   14292 api_server.go:88] waiting for apiserver healthz status ...
I0514 03:05:33.441007   14292 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:59278/healthz ...
I0514 03:05:33.445560   14292 out.go:177] * Enabled addons: default-storageclass, storage-provisioner
I0514 03:05:33.447517   14292 addons.go:514] duration metric: took 8.7660038s for enable addons: enabled=[default-storageclass storage-provisioner]
I0514 03:05:33.482996   14292 api_server.go:279] https://127.0.0.1:59278/healthz returned 200:
ok
I0514 03:05:33.487293   14292 api_server.go:141] control plane version: v1.32.0
I0514 03:05:33.487293   14292 api_server.go:131] duration metric: took 46.2859ms to wait for apiserver health ...
I0514 03:05:33.487293   14292 system_pods.go:43] waiting for kube-system pods to appear ...
I0514 03:05:33.515629   14292 system_pods.go:59] 7 kube-system pods found
I0514 03:05:33.515629   14292 system_pods.go:61] "coredns-668d6bf9bc-5tmrm" [eb3a1c11-4afd-4683-8e5a-912add5c8ed6] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0514 03:05:33.515629   14292 system_pods.go:61] "etcd-minikube" [869b787c-9483-4604-bfa9-555cf93be4cb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0514 03:05:33.515629   14292 system_pods.go:61] "kube-apiserver-minikube" [87238251-8f36-4545-9fdc-9fd9c4c29a40] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0514 03:05:33.515629   14292 system_pods.go:61] "kube-controller-manager-minikube" [6b69c5ea-13e2-47c9-8fcb-96ddab64399b] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0514 03:05:33.516184   14292 system_pods.go:61] "kube-proxy-lh5lp" [fb921fb7-414c-4c52-ab4d-e3212c8aaf12] Running
I0514 03:05:33.516184   14292 system_pods.go:61] "kube-scheduler-minikube" [812b81d2-6b78-4872-8922-f7b72e8cd48f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0514 03:05:33.516184   14292 system_pods.go:61] "storage-provisioner" [5965174f-b51e-491d-bad5-8743cdce20eb] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0514 03:05:33.516184   14292 system_pods.go:74] duration metric: took 28.8904ms to wait for pod list to return data ...
I0514 03:05:33.516283   14292 kubeadm.go:582] duration metric: took 8.8354849s to wait for: map[apiserver:true system_pods:true]
I0514 03:05:33.516283   14292 node_conditions.go:102] verifying NodePressure condition ...
I0514 03:05:33.526378   14292 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0514 03:05:33.526378   14292 node_conditions.go:123] node cpu capacity is 4
I0514 03:05:33.526378   14292 node_conditions.go:105] duration metric: took 10.0958ms to run NodePressure ...
I0514 03:05:33.526378   14292 start.go:241] waiting for startup goroutines ...
I0514 03:05:33.526378   14292 start.go:246] waiting for cluster config update ...
I0514 03:05:33.526378   14292 start.go:255] writing updated cluster config ...
I0514 03:05:33.565849   14292 ssh_runner.go:195] Run: rm -f paused
I0514 03:05:33.984686   14292 start.go:600] kubectl: 1.33.0, cluster: 1.32.0 (minor skew: 1)
I0514 03:05:33.985835   14292 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 13 22:05:10 minikube dockerd[7092]: time="2025-05-13T22:05:10.286247535Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 13 22:05:10 minikube dockerd[7092]: time="2025-05-13T22:05:10.287724309Z" level=info msg="Daemon shutdown complete"
May 13 22:05:10 minikube systemd[1]: docker.service: Deactivated successfully.
May 13 22:05:10 minikube systemd[1]: Stopped Docker Application Container Engine.
May 13 22:05:10 minikube systemd[1]: Starting Docker Application Container Engine...
May 13 22:05:10 minikube dockerd[7377]: time="2025-05-13T22:05:10.352097379Z" level=info msg="Starting up"
May 13 22:05:10 minikube dockerd[7377]: time="2025-05-13T22:05:10.354065342Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
May 13 22:05:10 minikube dockerd[7377]: time="2025-05-13T22:05:10.385906122Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 13 22:05:10 minikube dockerd[7377]: time="2025-05-13T22:05:10.441301261Z" level=info msg="Loading containers: start."
May 13 22:05:12 minikube dockerd[7377]: time="2025-05-13T22:05:12.731825044Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.355709947Z" level=info msg="Loading containers: done."
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460260797Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460362461Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460388002Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460400923Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460439685Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.460556473Z" level=info msg="Daemon has completed initialization"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.581008882Z" level=info msg="API listen on /var/run/docker.sock"
May 13 22:05:13 minikube dockerd[7377]: time="2025-05-13T22:05:13.581109243Z" level=info msg="API listen on [::]:2376"
May 13 22:05:13 minikube systemd[1]: Started Docker Application Container Engine.
May 13 22:05:13 minikube cri-dockerd[1640]: time="2025-05-13T22:05:13Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5tmrm_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4950887b05474dd8d3d09e9e554dbbef9adc7118e1bb2b836b1ba26efe8df9c1\""
May 13 22:05:13 minikube systemd[1]: Stopping CRI Interface for Docker Application Container Engine...
May 13 22:05:13 minikube systemd[1]: cri-docker.service: Deactivated successfully.
May 13 22:05:13 minikube systemd[1]: Stopped CRI Interface for Docker Application Container Engine.
May 13 22:05:15 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Start docker client with request timeout 0s"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Loaded network plugin cni"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Docker cri networking managed by network plugin cni"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Setting cgroupDriver cgroupfs"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Start cri-dockerd grpc backend"
May 13 22:05:15 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 13 22:05:15 minikube cri-dockerd[7684]: time="2025-05-13T22:05:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-5tmrm_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"4950887b05474dd8d3d09e9e554dbbef9adc7118e1bb2b836b1ba26efe8df9c1\""
May 13 22:05:17 minikube cri-dockerd[7684]: time="2025-05-13T22:05:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/64633563b441ee9fb7d887095b2f367f487fe5f6a7d691610528604886af543c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:17 minikube cri-dockerd[7684]: time="2025-05-13T22:05:17Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/04cae146d8d863e41a55eb8d1f6df999a19e560c4e1b14cf34aca96a2e20706e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:18 minikube cri-dockerd[7684]: time="2025-05-13T22:05:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/160df0daee173ef7d9b70726e4fc0a684431c0d52137bad095e38c94db8bf759/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:18 minikube cri-dockerd[7684]: time="2025-05-13T22:05:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e070a318b8957bea211066fd9730dbe25514d111526b79da5e26ffec2f70ff72/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:18 minikube cri-dockerd[7684]: time="2025-05-13T22:05:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/591cc04fffff6741e8d2a406150f4d4233a19419ae021a3022c770bce4a51feb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:18 minikube cri-dockerd[7684]: time="2025-05-13T22:05:18Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6dfe781b69cf7e52791e09edb7009bee925b0e430ef480e45904c7a0ec58c80f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:19 minikube cri-dockerd[7684]: time="2025-05-13T22:05:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/07d03f19d39f1bd452debdaa1ada71e734edc2d54dceffed2d7a60b33d6cb248/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 13 22:05:21 minikube dockerd[7377]: time="2025-05-13T22:05:21.010900568Z" level=info msg="ignoring event" container=f71bd37628f190d02f6d5a77866cf8a06161f2c2a2af2936dca0f2fc315282b6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 13 22:16:46 minikube dockerd[7377]: time="2025-05-13T22:16:46.926258290Z" level=error msg=/moby.buildkit.v1.frontend.LLBBridge/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=ea7af1c6635764a3 traceID=4d17834e9509b11dc5edb0fefcb9a00e
May 13 22:16:47 minikube dockerd[7377]: time="2025-05-13T22:16:47.018421407Z" level=error msg=/moby.buildkit.v1.Control/Solve error="rpc error: code = Unknown desc = failed to read dockerfile: open Dockerfile: no such file or directory" spanID=b002abc65dcd3ba1 traceID=4d17834e9509b11dc5edb0fefcb9a00e
May 13 22:27:25 minikube dockerd[7377]: 2025/05/13 22:27:25 http: TLS handshake error from 192.168.49.1:40464: read tcp 192.168.49.2:2376->192.168.49.1:40464: i/o timeout
May 13 22:43:27 minikube cri-dockerd[7684]: time="2025-05-13T22:43:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b751ab92471ad87ac38ec78d7149d55a72625b2973041655a4efc2fe5fde37ac/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 13 22:43:27 minikube cri-dockerd[7684]: time="2025-05-13T22:43:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1d98cabd58d88fd274f8e26840cf40150b7470430c764844e989525caaeaeef/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 13 22:43:30 minikube dockerd[7377]: time="2025-05-13T22:43:30.721055865Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 13 22:43:30 minikube dockerd[7377]: time="2025-05-13T22:43:30.721179079Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 22:43:33 minikube dockerd[7377]: time="2025-05-13T22:43:33.748123610Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 13 22:43:33 minikube dockerd[7377]: time="2025-05-13T22:43:33.748263826Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 22:43:47 minikube dockerd[7377]: time="2025-05-13T22:43:47.813474223Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 13 22:43:47 minikube dockerd[7377]: time="2025-05-13T22:43:47.813766456Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 22:43:50 minikube dockerd[7377]: time="2025-05-13T22:43:50.754056213Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 13 22:43:50 minikube dockerd[7377]: time="2025-05-13T22:43:50.754308841Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
May 13 22:44:15 minikube dockerd[7377]: time="2025-05-13T22:44:15.711856379Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
May 13 22:44:15 minikube dockerd[7377]: time="2025-05-13T22:44:15.711964888Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
0012aa1950562       6e38f40d628db       38 minutes ago      Running             storage-provisioner       3                   e070a318b8957       storage-provisioner
52b5607a94004       c69fa2e9cbf5f       39 minutes ago      Running             coredns                   1                   07d03f19d39f1       coredns-668d6bf9bc-5tmrm
eb6311145e1f3       a9e7e6b294baf       39 minutes ago      Running             etcd                      1                   591cc04fffff6       etcd-minikube
15a0ff502268e       c2e17b8d0f4a3       39 minutes ago      Running             kube-apiserver            1                   6dfe781b69cf7       kube-apiserver-minikube
66f27d3600619       a389e107f4ff1       39 minutes ago      Running             kube-scheduler            1                   160df0daee173       kube-scheduler-minikube
f71bd37628f19       6e38f40d628db       39 minutes ago      Exited              storage-provisioner       2                   e070a318b8957       storage-provisioner
c5afeb19c2282       8cab3d2a8bd0f       39 minutes ago      Running             kube-controller-manager   1                   04cae146d8d86       kube-controller-manager-minikube
f797fe73b4dd0       040f9f8aac8cd       39 minutes ago      Running             kube-proxy                1                   64633563b441e       kube-proxy-lh5lp
706e741926c0f       c69fa2e9cbf5f       53 minutes ago      Exited              coredns                   0                   4950887b05474       coredns-668d6bf9bc-5tmrm
8f959418a6459       040f9f8aac8cd       53 minutes ago      Exited              kube-proxy                0                   b752b9f648df1       kube-proxy-lh5lp
a09f001ed06b4       c2e17b8d0f4a3       53 minutes ago      Exited              kube-apiserver            0                   fecadb7183177       kube-apiserver-minikube
5914b1fde76d2       8cab3d2a8bd0f       53 minutes ago      Exited              kube-controller-manager   0                   63898c11bd62e       kube-controller-manager-minikube
a8c1a150b4846       a389e107f4ff1       53 minutes ago      Exited              kube-scheduler            0                   7015dc9074502       kube-scheduler-minikube
3e5068a16cc38       a9e7e6b294baf       53 minutes ago      Exited              etcd                      0                   9369a1c5a8068       etcd-minikube


==> coredns [52b5607a9400] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:42970 - 52754 "HINFO IN 1960241978404801608.1812937312606963409. udp 57 false 512" - - 0 6.020054305s
[ERROR] plugin/errors: 2 1960241978404801608.1812937312606963409. HINFO: read udp 10.244.0.4:45689->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:60881 - 5475 "HINFO IN 1960241978404801608.1812937312606963409. udp 57 false 512" - - 0 6.008055613s
[ERROR] plugin/errors: 2 1960241978404801608.1812937312606963409. HINFO: read udp 10.244.0.4:33978->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:56085 - 31185 "HINFO IN 1960241978404801608.1812937312606963409. udp 57 false 512" - - 0 4.001821586s
[ERROR] plugin/errors: 2 1960241978404801608.1812937312606963409. HINFO: read udp 10.244.0.4:48716->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:53558 - 64777 "HINFO IN 1960241978404801608.1812937312606963409. udp 57 false 512" - - 0 2.001287661s
[ERROR] plugin/errors: 2 1960241978404801608.1812937312606963409. HINFO: read udp 10.244.0.4:34017->192.168.65.254:53: i/o timeout
[INFO] 127.0.0.1:41974 - 4624 "HINFO IN 1960241978404801608.1812937312606963409. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.014849852s


==> coredns [706e741926c0] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:41527->192.168.65.254:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:58517->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:46222->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:59065->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:60361->192.168.65.254:53: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1403092078]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (13-May-2025 21:50:44.832) (total time: 21061ms):
Trace[1403092078]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21061ms (21:51:05.894)
Trace[1403092078]: [21.061817345s] [21.061817345s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[854468230]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (13-May-2025 21:50:44.833) (total time: 21065ms):
Trace[854468230]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21065ms (21:51:05.898)
Trace[854468230]: [21.0655639s] [21.0655639s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[465734598]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (13-May-2025 21:50:44.833) (total time: 21067ms):
Trace[465734598]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21066ms (21:51:05.900)
Trace[465734598]: [21.067451324s] [21.067451324s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/errors: 2 8113913491804866704.7689237026108219750. HINFO: read udp 10.244.0.3:46742->192.168.65.254:53: i/o timeout
[INFO] Reloading
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
[INFO] Reloading complete
[INFO] 127.0.0.1:55112 - 4215 "HINFO IN 3954794912732786041.6645044474625761262. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.014098141s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_14T02_50_35_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 13 May 2025 21:50:29 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 13 May 2025 22:44:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 13 May 2025 22:42:15 +0000   Tue, 13 May 2025 21:50:24 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 13 May 2025 22:42:15 +0000   Tue, 13 May 2025 21:50:24 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 13 May 2025 22:42:15 +0000   Tue, 13 May 2025 21:50:24 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 13 May 2025 22:42:15 +0000   Tue, 13 May 2025 21:50:29 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8092532Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8092532Ki
  pods:               110
System Info:
  Machine ID:                 5ed36ae8737d4ee2a60e5f4587d66539
  System UUID:                5ed36ae8737d4ee2a60e5f4587d66539
  Boot ID:                    7fd57928-7319-4873-afa6-90afb8fce810
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     my-app-566d4bf7cf-mwsgh             0 (0%)        0 (0%)      0 (0%)           0 (0%)         53s
  default                     my-app-566d4bf7cf-x4fpx             0 (0%)        0 (0%)      0 (0%)           0 (0%)         53s
  kube-system                 coredns-668d6bf9bc-5tmrm            100m (2%)     0 (0%)      70Mi (0%)        170Mi (2%)     53m
  kube-system                 etcd-minikube                       100m (2%)     0 (0%)      100Mi (1%)       0 (0%)         53m
  kube-system                 kube-apiserver-minikube             250m (6%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-controller-manager-minikube    200m (5%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-proxy-lh5lp                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 kube-scheduler-minikube             100m (2%)     0 (0%)      0 (0%)           0 (0%)         53m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           53m                kube-proxy       
  Normal   Starting                           38m                kube-proxy       
  Normal   NodeHasSufficientPID               54m (x7 over 54m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            54m                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           54m                kubelet          Starting kubelet.
  Warning  CgroupV1                           54m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            54m (x8 over 54m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              54m (x8 over 54m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Warning  PossibleMemoryBackedVolumesOnDisk  54m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           53m                kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  53m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Warning  CgroupV1                           53m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            53m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            53m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              53m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               53m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     53m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   CIDRAssignmentFailed               53m                cidrAllocator    Node minikube status is now: CIDRAssignmentFailed
  Normal   RegisteredNode                     38m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May13 19:11] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000077]  #2 #3
[  +0.200958] PCI: Fatal: No config space access function found
[  +0.225294] PCI: System does not support PCI
[  +0.124911] kvm: no hardware support
[  +0.000011] kvm: no hardware support
[  +5.935092] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.026064] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Karachi not found. Is the tzdata package installed?
[  +2.500538] netlink: 'init': attribute type 4 has an invalid length.
[  +4.215885] WSL (124) ERROR: CheckConnection: getaddrinfo() failed: -5
[May13 20:28] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Karachi not found. Is the tzdata package installed?
[  +0.049454] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Karachi not found. Is the tzdata package installed?
[  +0.002045] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Karachi not found. Is the tzdata package installed?
[May13 21:50] tmpfs: Unknown parameter 'noswap'
[ +16.593703] tmpfs: Unknown parameter 'noswap'
[May13 22:14] hrtimer: interrupt took 751100 ns


==> etcd [3e5068a16cc3] <==
{"level":"info","ts":"2025-05-13T21:50:22.001671Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-13T21:50:22.001739Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-13T21:50:29.432960Z","caller":"traceutil/trace.go:171","msg":"trace[278930755] transaction","detail":"{read_only:false; response_revision:15; number_of_response:1; }","duration":"120.22762ms","start":"2025-05-13T21:50:29.312693Z","end":"2025-05-13T21:50:29.432920Z","steps":["trace[278930755] 'process raft request'  (duration: 33.244738ms)","trace[278930755] 'compare'  (duration: 86.611011ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:29.433303Z","caller":"traceutil/trace.go:171","msg":"trace[1678050565] transaction","detail":"{read_only:false; response_revision:16; number_of_response:1; }","duration":"111.794813ms","start":"2025-05-13T21:50:29.321485Z","end":"2025-05-13T21:50:29.433280Z","steps":["trace[1678050565] 'process raft request'  (duration: 111.325023ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:29.432992Z","caller":"traceutil/trace.go:171","msg":"trace[957750464] linearizableReadLoop","detail":"{readStateIndex:18; appliedIndex:17; }","duration":"120.142405ms","start":"2025-05-13T21:50:29.312827Z","end":"2025-05-13T21:50:29.432969Z","steps":["trace[957750464] 'read index received'  (duration: 8.153255ms)","trace[957750464] 'applied index is now lower than readState.Index'  (duration: 111.987949ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T21:50:29.434727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.849029ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-05-13T21:50:29.435491Z","caller":"traceutil/trace.go:171","msg":"trace[791329699] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:0; response_revision:19; }","duration":"122.684989ms","start":"2025-05-13T21:50:29.312773Z","end":"2025-05-13T21:50:29.435458Z","steps":["trace[791329699] 'agreement among raft nodes before linearized reading'  (duration: 121.818224ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:31.269472Z","caller":"traceutil/trace.go:171","msg":"trace[2146948805] transaction","detail":"{read_only:false; response_revision:174; number_of_response:1; }","duration":"249.94675ms","start":"2025-05-13T21:50:31.019488Z","end":"2025-05-13T21:50:31.269434Z","steps":["trace[2146948805] 'process raft request'  (duration: 180.245362ms)","trace[2146948805] 'compare'  (duration: 69.39643ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:31.329986Z","caller":"traceutil/trace.go:171","msg":"trace[1145769480] transaction","detail":"{read_only:false; response_revision:175; number_of_response:1; }","duration":"110.541774ms","start":"2025-05-13T21:50:31.219416Z","end":"2025-05-13T21:50:31.329958Z","steps":["trace[1145769480] 'process raft request'  (duration: 110.319632ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:31.464860Z","caller":"traceutil/trace.go:171","msg":"trace[1640052009] transaction","detail":"{read_only:false; response_revision:177; number_of_response:1; }","duration":"111.306419ms","start":"2025-05-13T21:50:31.353522Z","end":"2025-05-13T21:50:31.464828Z","steps":["trace[1640052009] 'process raft request'  (duration: 60.181473ms)","trace[1640052009] 'compare'  (duration: 50.901104ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:31.994879Z","caller":"traceutil/trace.go:171","msg":"trace[712169280] transaction","detail":"{read_only:false; response_revision:192; number_of_response:1; }","duration":"151.781336ms","start":"2025-05-13T21:50:31.843070Z","end":"2025-05-13T21:50:31.994851Z","steps":["trace[712169280] 'process raft request'  (duration: 134.339211ms)","trace[712169280] 'compare'  (duration: 17.278494ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:32.211327Z","caller":"traceutil/trace.go:171","msg":"trace[1433635163] transaction","detail":"{read_only:false; response_revision:196; number_of_response:1; }","duration":"122.430841ms","start":"2025-05-13T21:50:32.088864Z","end":"2025-05-13T21:50:32.211295Z","steps":["trace[1433635163] 'process raft request'  (duration: 114.604449ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:35.211537Z","caller":"traceutil/trace.go:171","msg":"trace[1135908563] linearizableReadLoop","detail":"{readStateIndex:293; appliedIndex:292; }","duration":"116.563441ms","start":"2025-05-13T21:50:35.094942Z","end":"2025-05-13T21:50:35.211506Z","steps":["trace[1135908563] 'read index received'  (duration: 50.656644ms)","trace[1135908563] 'applied index is now lower than readState.Index'  (duration: 65.905397ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:35.212010Z","caller":"traceutil/trace.go:171","msg":"trace[873943263] transaction","detail":"{read_only:false; response_revision:287; number_of_response:1; }","duration":"118.638215ms","start":"2025-05-13T21:50:35.093320Z","end":"2025-05-13T21:50:35.211958Z","steps":["trace[873943263] 'process raft request'  (duration: 52.156914ms)","trace[873943263] 'compare'  (duration: 65.618645ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T21:50:35.212840Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.870776ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:1 size:351"}
{"level":"info","ts":"2025-05-13T21:50:35.213081Z","caller":"traceutil/trace.go:171","msg":"trace[400337773] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:1; response_revision:287; }","duration":"118.155628ms","start":"2025-05-13T21:50:35.094896Z","end":"2025-05-13T21:50:35.213052Z","steps":["trace[400337773] 'agreement among raft nodes before linearized reading'  (duration: 117.785061ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T21:50:37.229377Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"177.084466ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037228813541564 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/pods/kube-system/kube-apiserver-minikube\" mod_revision:0 > success:<request_put:<key:\"/registry/pods/kube-system/kube-apiserver-minikube\" value_size:5981 >> failure:<>>","response":"size:16"}
{"level":"info","ts":"2025-05-13T21:50:37.242090Z","caller":"traceutil/trace.go:171","msg":"trace[237966211] linearizableReadLoop","detail":"{readStateIndex:317; appliedIndex:316; }","duration":"238.393932ms","start":"2025-05-13T21:50:37.003663Z","end":"2025-05-13T21:50:37.242057Z","steps":["trace[237966211] 'read index received'  (duration: 48.018568ms)","trace[237966211] 'applied index is now lower than readState.Index'  (duration: 190.371263ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T21:50:37.242384Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"238.670982ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/token-cleaner\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T21:50:37.242448Z","caller":"traceutil/trace.go:171","msg":"trace[1357438969] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/token-cleaner; range_end:; response_count:0; response_revision:311; }","duration":"238.807906ms","start":"2025-05-13T21:50:37.003622Z","end":"2025-05-13T21:50:37.242430Z","steps":["trace[1357438969] 'agreement among raft nodes before linearized reading'  (duration: 238.545859ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:37.242719Z","caller":"traceutil/trace.go:171","msg":"trace[2138257666] transaction","detail":"{read_only:false; response_revision:310; number_of_response:1; }","duration":"279.734794ms","start":"2025-05-13T21:50:36.962964Z","end":"2025-05-13T21:50:37.242699Z","steps":["trace[2138257666] 'process raft request'  (duration: 88.732917ms)","trace[2138257666] 'compare'  (duration: 176.656187ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:37.292996Z","caller":"traceutil/trace.go:171","msg":"trace[467702878] transaction","detail":"{read_only:false; response_revision:311; number_of_response:1; }","duration":"250.004928ms","start":"2025-05-13T21:50:37.042958Z","end":"2025-05-13T21:50:37.292962Z","steps":["trace[467702878] 'process raft request'  (duration: 186.537771ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:39.637388Z","caller":"traceutil/trace.go:171","msg":"trace[2117419552] transaction","detail":"{read_only:false; response_revision:338; number_of_response:1; }","duration":"118.366266ms","start":"2025-05-13T21:50:39.518990Z","end":"2025-05-13T21:50:39.637356Z","steps":["trace[2117419552] 'process raft request'  (duration: 104.20591ms)","trace[2117419552] 'compare'  (duration: 13.147773ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:39.637721Z","caller":"traceutil/trace.go:171","msg":"trace[531621606] transaction","detail":"{read_only:false; response_revision:339; number_of_response:1; }","duration":"118.661719ms","start":"2025-05-13T21:50:39.519036Z","end":"2025-05-13T21:50:39.637698Z","steps":["trace[531621606] 'process raft request'  (duration: 117.460302ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:39.638183Z","caller":"traceutil/trace.go:171","msg":"trace[681670906] transaction","detail":"{read_only:false; response_revision:340; number_of_response:1; }","duration":"109.869533ms","start":"2025-05-13T21:50:39.528290Z","end":"2025-05-13T21:50:39.638160Z","steps":["trace[681670906] 'process raft request'  (duration: 108.297449ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:39.638485Z","caller":"traceutil/trace.go:171","msg":"trace[693649869] linearizableReadLoop","detail":"{readStateIndex:348; appliedIndex:343; }","duration":"106.448615ms","start":"2025-05-13T21:50:39.531995Z","end":"2025-05-13T21:50:39.638443Z","steps":["trace[693649869] 'read index received'  (duration: 81.304376ms)","trace[693649869] 'applied index is now lower than readState.Index'  (duration: 25.141338ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:39.638775Z","caller":"traceutil/trace.go:171","msg":"trace[2073230940] transaction","detail":"{read_only:false; response_revision:341; number_of_response:1; }","duration":"102.068625ms","start":"2025-05-13T21:50:39.536678Z","end":"2025-05-13T21:50:39.638747Z","steps":["trace[2073230940] 'process raft request'  (duration: 99.99115ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T21:50:39.640130Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"121.167471ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/certificate-controller\" limit:1 ","response":"range_response_count:1 size:209"}
{"level":"info","ts":"2025-05-13T21:50:39.640218Z","caller":"traceutil/trace.go:171","msg":"trace[766498877] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/certificate-controller; range_end:; response_count:1; response_revision:343; }","duration":"121.288494ms","start":"2025-05-13T21:50:39.518905Z","end":"2025-05-13T21:50:39.640194Z","steps":["trace[766498877] 'agreement among raft nodes before linearized reading'  (duration: 121.124064ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.113452Z","caller":"traceutil/trace.go:171","msg":"trace[1086057337] transaction","detail":"{read_only:false; response_revision:356; number_of_response:1; }","duration":"112.491505ms","start":"2025-05-13T21:50:40.000910Z","end":"2025-05-13T21:50:40.113401Z","steps":["trace[1086057337] 'process raft request'  (duration: 112.317874ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.114722Z","caller":"traceutil/trace.go:171","msg":"trace[148140543] transaction","detail":"{read_only:false; response_revision:354; number_of_response:1; }","duration":"116.235682ms","start":"2025-05-13T21:50:39.998424Z","end":"2025-05-13T21:50:40.114660Z","steps":["trace[148140543] 'process raft request'  (duration: 26.831943ms)","trace[148140543] 'compare'  (duration: 87.403177ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:40.115428Z","caller":"traceutil/trace.go:171","msg":"trace[1584593788] transaction","detail":"{read_only:false; number_of_response:1; response_revision:354; }","duration":"116.087555ms","start":"2025-05-13T21:50:39.999185Z","end":"2025-05-13T21:50:40.115272Z","steps":["trace[1584593788] 'process raft request'  (duration: 113.758335ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.116008Z","caller":"traceutil/trace.go:171","msg":"trace[1838995401] transaction","detail":"{read_only:false; response_revision:355; number_of_response:1; }","duration":"115.935627ms","start":"2025-05-13T21:50:40.000023Z","end":"2025-05-13T21:50:40.115959Z","steps":["trace[1838995401] 'process raft request'  (duration: 113.088513ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.126609Z","caller":"traceutil/trace.go:171","msg":"trace[1293559488] transaction","detail":"{read_only:false; response_revision:357; number_of_response:1; }","duration":"101.467916ms","start":"2025-05-13T21:50:40.025088Z","end":"2025-05-13T21:50:40.126556Z","steps":["trace[1293559488] 'process raft request'  (duration: 101.255278ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.204367Z","caller":"traceutil/trace.go:171","msg":"trace[565505818] transaction","detail":"{read_only:false; response_revision:358; number_of_response:1; }","duration":"102.445093ms","start":"2025-05-13T21:50:40.101890Z","end":"2025-05-13T21:50:40.204335Z","steps":["trace[565505818] 'process raft request'  (duration: 102.274862ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:40.809356Z","caller":"traceutil/trace.go:171","msg":"trace[1763931678] transaction","detail":"{read_only:false; response_revision:377; number_of_response:1; }","duration":"103.490981ms","start":"2025-05-13T21:50:40.705812Z","end":"2025-05-13T21:50:40.809303Z","steps":["trace[1763931678] 'process raft request'  (duration: 28.956108ms)","trace[1763931678] 'compare'  (duration: 73.773365ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T21:50:41.288901Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"368.182029ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/storageclasses/standard\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T21:50:41.288977Z","caller":"traceutil/trace.go:171","msg":"trace[211122131] range","detail":"{range_begin:/registry/storageclasses/standard; range_end:; response_count:0; response_revision:377; }","duration":"379.112078ms","start":"2025-05-13T21:50:40.909848Z","end":"2025-05-13T21:50:41.288960Z","steps":["trace[211122131] 'range keys from in-memory index tree'  (duration: 367.817976ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T21:50:41.289019Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T21:50:40.909812Z","time spent":"379.196391ms","remote":"127.0.0.1:49752","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":0,"response size":29,"request content":"key:\"/registry/storageclasses/standard\" limit:1 "}
{"level":"info","ts":"2025-05-13T21:50:41.289582Z","caller":"traceutil/trace.go:171","msg":"trace[1778589800] transaction","detail":"{read_only:false; response_revision:378; number_of_response:1; }","duration":"184.126119ms","start":"2025-05-13T21:50:41.105433Z","end":"2025-05-13T21:50:41.289559Z","steps":["trace[1778589800] 'process raft request'  (duration: 180.134153ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:41.728628Z","caller":"traceutil/trace.go:171","msg":"trace[373811725] transaction","detail":"{read_only:false; response_revision:386; number_of_response:1; }","duration":"108.770629ms","start":"2025-05-13T21:50:41.619832Z","end":"2025-05-13T21:50:41.728603Z","steps":["trace[373811725] 'process raft request'  (duration: 108.673415ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:50:41.729135Z","caller":"traceutil/trace.go:171","msg":"trace[1814237489] transaction","detail":"{read_only:false; response_revision:385; number_of_response:1; }","duration":"109.884688ms","start":"2025-05-13T21:50:41.619221Z","end":"2025-05-13T21:50:41.729106Z","steps":["trace[1814237489] 'process raft request'  (duration: 23.217993ms)","trace[1814237489] 'compare'  (duration: 85.906087ms)"],"step_count":2}
{"level":"info","ts":"2025-05-13T21:50:41.792890Z","caller":"traceutil/trace.go:171","msg":"trace[2012340710] transaction","detail":"{read_only:false; response_revision:387; number_of_response:1; }","duration":"104.153474ms","start":"2025-05-13T21:50:41.688707Z","end":"2025-05-13T21:50:41.792861Z","steps":["trace[2012340710] 'process raft request'  (duration: 104.031757ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T21:52:22.892361Z","caller":"traceutil/trace.go:171","msg":"trace[541784339] transaction","detail":"{read_only:false; response_revision:517; number_of_response:1; }","duration":"116.805723ms","start":"2025-05-13T21:52:22.775531Z","end":"2025-05-13T21:52:22.892337Z","steps":["trace[541784339] 'process raft request'  (duration: 116.653095ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:00:24.153454Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":662}
{"level":"info","ts":"2025-05-13T22:00:24.172530Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":662,"took":"18.099211ms","hash":632455164,"current-db-size-bytes":2080768,"current-db-size":"2.1 MB","current-db-size-in-use-bytes":2080768,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-05-13T22:00:24.172839Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":632455164,"revision":662,"compact-revision":-1}
{"level":"info","ts":"2025-05-13T22:02:06.691094Z","caller":"traceutil/trace.go:171","msg":"trace[2096497072] transaction","detail":"{read_only:false; response_revision:982; number_of_response:1; }","duration":"105.683445ms","start":"2025-05-13T22:02:06.585376Z","end":"2025-05-13T22:02:06.691060Z","steps":["trace[2096497072] 'process raft request'  (duration: 105.376692ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:04:46.170897Z","caller":"traceutil/trace.go:171","msg":"trace[541183721] transaction","detail":"{read_only:false; response_revision:1108; number_of_response:1; }","duration":"105.161334ms","start":"2025-05-13T22:04:46.065703Z","end":"2025-05-13T22:04:46.170864Z","steps":["trace[541183721] 'process raft request'  (duration: 104.664958ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:04:56.694872Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-13T22:04:56.694981Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-05-13T22:04:56.695191Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
2025/05/13 22:04:56 WARNING: [core] [Server #5] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-13T22:04:56.712920Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-13T22:04:56.789150Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-13T22:04:56.789635Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-13T22:04:56.790691Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-13T22:04:56.833235Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-13T22:04:56.833598Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-13T22:04:56.833658Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [eb6311145e1f] <==
{"level":"warn","ts":"2025-05-13T22:20:58.208798Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:56.211273Z","time spent":"1.997509764s","remote":"127.0.0.1:52202","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":0,"request content":"key:\"/registry/health\" "}
2025/05/13 22:20:58 WARNING: [core] [Server #8] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2025-05-13T22:20:58.232279Z","caller":"wal/wal.go:805","msg":"slow fdatasync","took":"1.572648873s","expected-duration":"1s"}
{"level":"info","ts":"2025-05-13T22:20:58.232738Z","caller":"traceutil/trace.go:171","msg":"trace[96046135] transaction","detail":"{read_only:false; response_revision:1973; number_of_response:1; }","duration":"4.714772846s","start":"2025-05-13T22:20:53.517933Z","end":"2025-05-13T22:20:58.232706Z","steps":["trace[96046135] 'process raft request'  (duration: 4.71463423s)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.232889Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:53.517870Z","time spent":"4.714957067s","remote":"127.0.0.1:52466","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:1967 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"warn","ts":"2025-05-13T22:20:58.418150Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"155.938311ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037229044129100 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1968 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-05-13T22:20:58.418295Z","caller":"traceutil/trace.go:171","msg":"trace[449441534] transaction","detail":"{read_only:false; response_revision:1974; number_of_response:1; }","duration":"4.713458591s","start":"2025-05-13T22:20:53.704812Z","end":"2025-05-13T22:20:58.418271Z","steps":["trace[449441534] 'process raft request'  (duration: 4.557314056s)","trace[449441534] 'compare'  (duration: 155.769291ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T22:20:58.418406Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:53.704782Z","time spent":"4.713559604s","remote":"127.0.0.1:52466","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1968 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"info","ts":"2025-05-13T22:20:58.418509Z","caller":"traceutil/trace.go:171","msg":"trace[16371881] linearizableReadLoop","detail":"{readStateIndex:2352; appliedIndex:2350; }","duration":"1.72625411s","start":"2025-05-13T22:20:56.692237Z","end":"2025-05-13T22:20:58.418491Z","steps":["trace[16371881] 'read index received'  (duration: 1.541078666s)","trace[16371881] 'applied index is now lower than readState.Index'  (duration: 185.174544ms)"],"step_count":2}
{"level":"warn","ts":"2025-05-13T22:20:58.418608Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:56.693688Z","time spent":"1.724916654s","remote":"127.0.0.1:52226","response type":"/etcdserverpb.Lease/LeaseGrant","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2025-05-13T22:20:58.418741Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"4.024452683s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"warn","ts":"2025-05-13T22:20:58.418793Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.730467605s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:20:58.418801Z","caller":"traceutil/trace.go:171","msg":"trace[225564580] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:1974; }","duration":"4.024563196s","start":"2025-05-13T22:20:54.394220Z","end":"2025-05-13T22:20:58.418783Z","steps":["trace[225564580] 'agreement among raft nodes before linearized reading'  (duration: 4.024439681s)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:20:58.418837Z","caller":"traceutil/trace.go:171","msg":"trace[854871237] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1974; }","duration":"1.730517611s","start":"2025-05-13T22:20:56.688306Z","end":"2025-05-13T22:20:58.418824Z","steps":["trace[854871237] 'agreement among raft nodes before linearized reading'  (duration: 1.730446703s)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.418856Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:54.394200Z","time spent":"4.024638905s","remote":"127.0.0.1:52364","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"warn","ts":"2025-05-13T22:20:58.418919Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"817.489997ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:20:58.418954Z","caller":"traceutil/trace.go:171","msg":"trace[670965599] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1974; }","duration":"817.585107ms","start":"2025-05-13T22:20:57.601357Z","end":"2025-05-13T22:20:58.418942Z","steps":["trace[670965599] 'agreement among raft nodes before linearized reading'  (duration: 817.536602ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.418990Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:57.601315Z","time spent":"817.665417ms","remote":"127.0.0.1:52206","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-05-13T22:20:58.419069Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.939362235s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2025-05-13T22:20:58.418857Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"4.225154551s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-05-13T22:20:58.419112Z","caller":"traceutil/trace.go:171","msg":"trace[755603277] range","detail":"{range_begin:/registry/daemonsets/; range_end:/registry/daemonsets0; response_count:0; response_revision:1974; }","duration":"1.939478749s","start":"2025-05-13T22:20:56.479619Z","end":"2025-05-13T22:20:58.419097Z","steps":["trace[755603277] 'agreement among raft nodes before linearized reading'  (duration: 1.93940904s)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:20:58.419116Z","caller":"traceutil/trace.go:171","msg":"trace[1507820592] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices/; range_end:/registry/apiregistration.k8s.io/apiservices0; response_count:0; response_revision:1974; }","duration":"4.225453986s","start":"2025-05-13T22:20:54.193649Z","end":"2025-05-13T22:20:58.419103Z","steps":["trace[1507820592] 'agreement among raft nodes before linearized reading'  (duration: 4.225160252s)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.419152Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:56.479576Z","time spent":"1.939563358s","remote":"127.0.0.1:52634","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":1,"response size":31,"request content":"key:\"/registry/daemonsets/\" range_end:\"/registry/daemonsets0\" count_only:true "}
{"level":"warn","ts":"2025-05-13T22:20:58.419165Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:54.193626Z","time spent":"4.225521694s","remote":"127.0.0.1:52688","response type":"/etcdserverpb.KV/Range","request count":0,"request size":96,"response count":20,"response size":31,"request content":"key:\"/registry/apiregistration.k8s.io/apiservices/\" range_end:\"/registry/apiregistration.k8s.io/apiservices0\" count_only:true "}
{"level":"warn","ts":"2025-05-13T22:20:58.419222Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"200.88809ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:20:58.419262Z","caller":"traceutil/trace.go:171","msg":"trace[434803794] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1974; }","duration":"200.965199ms","start":"2025-05-13T22:20:58.218283Z","end":"2025-05-13T22:20:58.419248Z","steps":["trace[434803794] 'agreement among raft nodes before linearized reading'  (duration: 200.908593ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.419439Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"207.211832ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/kube-system/kube-apiserver-minikube.183f362f4816bfb4\" limit:1 ","response":"range_response_count:1 size:774"}
{"level":"info","ts":"2025-05-13T22:20:58.419475Z","caller":"traceutil/trace.go:171","msg":"trace[1066225095] range","detail":"{range_begin:/registry/events/kube-system/kube-apiserver-minikube.183f362f4816bfb4; range_end:; response_count:1; response_revision:1974; }","duration":"207.285041ms","start":"2025-05-13T22:20:58.212178Z","end":"2025-05-13T22:20:58.419464Z","steps":["trace[1066225095] 'agreement among raft nodes before linearized reading'  (duration: 207.213133ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.419797Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"610.75522ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:20:58.419850Z","caller":"traceutil/trace.go:171","msg":"trace[902446033] range","detail":"{range_begin:/registry/validatingadmissionpolicybindings/; range_end:/registry/validatingadmissionpolicybindings0; response_count:0; response_revision:1974; }","duration":"610.876134ms","start":"2025-05-13T22:20:57.808961Z","end":"2025-05-13T22:20:58.419837Z","steps":["trace[902446033] 'agreement among raft nodes before linearized reading'  (duration: 610.789424ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:20:58.419903Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:20:57.808915Z","time spent":"610.970945ms","remote":"127.0.0.1:52676","response type":"/etcdserverpb.KV/Range","request count":0,"request size":94,"response count":0,"response size":29,"request content":"key:\"/registry/validatingadmissionpolicybindings/\" range_end:\"/registry/validatingadmissionpolicybindings0\" count_only:true "}
{"level":"warn","ts":"2025-05-13T22:21:18.125389Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"533.007964ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-05-13T22:21:18.125401Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"832.067767ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:21:18.125472Z","caller":"traceutil/trace.go:171","msg":"trace[790654370] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1992; }","duration":"533.136284ms","start":"2025-05-13T22:21:17.592314Z","end":"2025-05-13T22:21:18.125451Z","steps":["trace[790654370] 'range keys from in-memory index tree'  (duration: 532.884344ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:21:18.125488Z","caller":"traceutil/trace.go:171","msg":"trace[1473721912] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1992; }","duration":"832.155981ms","start":"2025-05-13T22:21:17.293304Z","end":"2025-05-13T22:21:18.125460Z","steps":["trace[1473721912] 'range keys from in-memory index tree'  (duration: 832.054064ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:21:18.125523Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-05-13T22:21:17.592294Z","time spent":"533.215397ms","remote":"127.0.0.1:52206","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2025-05-13T22:23:13.144437Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"235.84ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-05-13T22:23:13.144535Z","caller":"traceutil/trace.go:171","msg":"trace[256781825] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:2085; }","duration":"235.991698ms","start":"2025-05-13T22:23:12.908521Z","end":"2025-05-13T22:23:13.144513Z","steps":["trace[256781825] 'range keys from in-memory index tree'  (duration: 235.112111ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:23:34.559427Z","caller":"traceutil/trace.go:171","msg":"trace[1285087466] transaction","detail":"{read_only:false; response_revision:2102; number_of_response:1; }","duration":"134.134134ms","start":"2025-05-13T22:23:34.425267Z","end":"2025-05-13T22:23:34.559401Z","steps":["trace[1285087466] 'process raft request'  (duration: 133.912003ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:25:19.044754Z","caller":"traceutil/trace.go:171","msg":"trace[1174790255] transaction","detail":"{read_only:false; response_revision:2183; number_of_response:1; }","duration":"119.402068ms","start":"2025-05-13T22:25:18.925323Z","end":"2025-05-13T22:25:19.044726Z","steps":["trace[1174790255] 'process raft request'  (duration: 119.22063ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:25:33.121487Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1956}
{"level":"info","ts":"2025-05-13T22:25:33.182631Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1956,"took":"50.413195ms","hash":136976600,"current-db-size-bytes":2719744,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1429504,"current-db-size-in-use":"1.4 MB"}
{"level":"info","ts":"2025-05-13T22:25:33.182856Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":136976600,"revision":1956,"compact-revision":1724}
{"level":"info","ts":"2025-05-13T22:25:42.180677Z","caller":"traceutil/trace.go:171","msg":"trace[158951704] transaction","detail":"{read_only:false; response_revision:2201; number_of_response:1; }","duration":"192.953215ms","start":"2025-05-13T22:25:41.987690Z","end":"2025-05-13T22:25:42.180643Z","steps":["trace[158951704] 'process raft request'  (duration: 192.715255ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:25:53.082417Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"159.554226ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/default/kubernetes\" limit:1 ","response":"range_response_count:1 size:474"}
{"level":"info","ts":"2025-05-13T22:25:53.082724Z","caller":"traceutil/trace.go:171","msg":"trace[106309653] range","detail":"{range_begin:/registry/endpointslices/default/kubernetes; range_end:; response_count:1; response_revision:2210; }","duration":"159.960316ms","start":"2025-05-13T22:25:52.922735Z","end":"2025-05-13T22:25:53.082696Z","steps":["trace[106309653] 'range keys from in-memory index tree'  (duration: 158.996465ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:29:40.264156Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"133.565012ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-13T22:29:40.264257Z","caller":"traceutil/trace.go:171","msg":"trace[1111613646] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:2390; }","duration":"133.679321ms","start":"2025-05-13T22:29:40.130556Z","end":"2025-05-13T22:29:40.264235Z","steps":["trace[1111613646] 'range keys from in-memory index tree'  (duration: 133.215885ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:30:33.122366Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2194}
{"level":"info","ts":"2025-05-13T22:30:33.135976Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2194,"took":"12.542724ms","hash":3230714079,"current-db-size-bytes":2719744,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1470464,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-05-13T22:30:33.136122Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3230714079,"revision":2194,"compact-revision":1956}
{"level":"info","ts":"2025-05-13T22:31:41.960592Z","caller":"traceutil/trace.go:171","msg":"trace[1113520787] transaction","detail":"{read_only:false; response_revision:2486; number_of_response:1; }","duration":"153.541121ms","start":"2025-05-13T22:31:41.807023Z","end":"2025-05-13T22:31:41.960564Z","steps":["trace[1113520787] 'process raft request'  (duration: 153.224273ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-13T22:32:47.983648Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.683514ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128037229044132557 > lease_revoke:<id:70cc96cbae536a81>","response":"size:29"}
{"level":"info","ts":"2025-05-13T22:35:33.098854Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2433}
{"level":"info","ts":"2025-05-13T22:35:33.103438Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2433,"took":"3.489259ms","hash":1912984370,"current-db-size-bytes":2719744,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1490944,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-05-13T22:35:33.103544Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1912984370,"revision":2433,"compact-revision":2194}
{"level":"info","ts":"2025-05-13T22:40:23.580326Z","caller":"traceutil/trace.go:171","msg":"trace[1623571825] transaction","detail":"{read_only:false; response_revision:2903; number_of_response:1; }","duration":"118.947741ms","start":"2025-05-13T22:40:23.461342Z","end":"2025-05-13T22:40:23.580290Z","steps":["trace[1623571825] 'process raft request'  (duration: 118.600898ms)"],"step_count":1}
{"level":"info","ts":"2025-05-13T22:40:33.074229Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2672}
{"level":"info","ts":"2025-05-13T22:40:33.078988Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":2672,"took":"4.365731ms","hash":223020597,"current-db-size-bytes":2719744,"current-db-size":"2.7 MB","current-db-size-in-use-bytes":1470464,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-05-13T22:40:33.079074Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":223020597,"revision":2672,"compact-revision":2433}


==> kernel <==
 22:44:19 up  3:32,  0 users,  load average: 0.77, 0.97, 1.53
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [15a0ff502268] <==
I0513 22:05:29.176526       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0513 22:05:29.176713       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0513 22:05:29.176573       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0513 22:05:29.177442       1 secure_serving.go:213] Serving securely on [::]:8443
I0513 22:05:29.177623       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0513 22:05:29.191078       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0513 22:05:29.191479       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0513 22:05:29.191593       1 aggregator.go:169] waiting for initial CRD sync...
I0513 22:05:29.191587       1 local_available_controller.go:156] Starting LocalAvailability controller
I0513 22:05:29.191626       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I0513 22:05:29.228922       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0513 22:05:29.228995       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0513 22:05:29.229059       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0513 22:05:29.229074       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0513 22:05:29.229174       1 controller.go:78] Starting OpenAPI AggregationController
I0513 22:05:29.229529       1 controller.go:142] Starting OpenAPI controller
I0513 22:05:29.229596       1 controller.go:90] Starting OpenAPI V3 controller
I0513 22:05:29.229707       1 naming_controller.go:294] Starting NamingConditionController
I0513 22:05:29.229758       1 establishing_controller.go:81] Starting EstablishingController
I0513 22:05:29.229787       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0513 22:05:29.229826       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0513 22:05:29.229851       1 crd_finalizer.go:269] Starting CRDFinalizer
I0513 22:05:29.229894       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0513 22:05:29.229908       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0513 22:05:29.229949       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0513 22:05:29.230126       1 controller.go:119] Starting legacy_token_tracking_controller
I0513 22:05:29.230142       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0513 22:05:29.231142       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0513 22:05:29.231328       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0513 22:05:29.231407       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0513 22:05:29.233886       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0513 22:05:29.233920       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0513 22:05:29.233966       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0513 22:05:29.234105       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0513 22:05:29.432949       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0513 22:05:29.452979       1 aggregator.go:171] initial CRD sync complete...
I0513 22:05:29.453018       1 autoregister_controller.go:144] Starting autoregister controller
I0513 22:05:29.453031       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0513 22:05:29.453043       1 cache.go:39] Caches are synced for autoregister controller
I0513 22:05:29.509772       1 cache.go:39] Caches are synced for LocalAvailability controller
I0513 22:05:29.510768       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0513 22:05:29.510821       1 policy_source.go:240] refreshing policies
I0513 22:05:29.543844       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0513 22:05:29.544132       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0513 22:05:29.548990       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0513 22:05:29.553455       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0513 22:05:29.557432       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0513 22:05:29.557498       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0513 22:05:29.557705       1 shared_informer.go:320] Caches are synced for configmaps
I0513 22:05:29.564373       1 shared_informer.go:320] Caches are synced for node_authorizer
I0513 22:05:29.566618       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0513 22:05:30.314294       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0513 22:05:32.865611       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0513 22:05:32.868164       1 controller.go:615] quota admission added evaluator for: endpoints
I0513 22:05:32.887766       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0513 22:05:33.464338       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0513 22:05:34.835014       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0513 22:05:35.112943       1 controller.go:615] quota admission added evaluator for: deployments.apps
E0513 22:19:55.518919       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: rpctypes.EtcdError{code:0xe, desc:\"etcdserver: request timed out\"}: etcdserver: request timed out" logger="UnhandledError"
I0513 22:43:27.108616       1 alloc.go:330] "allocated clusterIPs" service="default/my-app-service" clusterIPs={"IPv4":"10.109.120.234"}


==> kube-apiserver [a09f001ed06b] <==
W0513 22:05:02.231468       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.239795       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.241849       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.261991       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.262207       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.267456       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.275053       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.361264       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.374848       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.462495       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.465765       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.514124       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:02.644160       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:04.644777       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:04.677019       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:04.734809       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:04.760035       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:04.951710       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.078190       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.138818       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.155406       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.217083       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.308921       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.334783       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.350660       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.356567       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.391963       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.403983       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.427613       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.463499       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.532965       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.639093       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.650784       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.749239       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.826717       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.839648       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.842234       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.852986       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.888768       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.914392       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:05.977323       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.002190       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.003513       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.003974       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.012535       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.017914       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.142863       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.176098       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.220724       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.248619       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.300049       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.336080       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.343888       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.382487       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.534831       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.590310       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.600936       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.676301       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.686076       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0513 22:05:06.696622       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [5914b1fde76d] <==
I0513 21:50:39.033210       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 21:50:39.036178       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0513 21:50:39.036208       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0513 21:50:39.033245       1 shared_informer.go:320] Caches are synced for deployment
I0513 21:50:39.039461       1 shared_informer.go:320] Caches are synced for resource quota
I0513 21:50:39.093613       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0513 21:50:39.093716       1 shared_informer.go:320] Caches are synced for endpoint
I0513 21:50:39.093825       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0513 21:50:39.094786       1 shared_informer.go:320] Caches are synced for attach detach
I0513 21:50:39.095386       1 shared_informer.go:320] Caches are synced for daemon sets
I0513 21:50:39.104155       1 shared_informer.go:320] Caches are synced for ReplicationController
I0513 21:50:39.105446       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0513 21:50:39.106562       1 shared_informer.go:320] Caches are synced for ephemeral
I0513 21:50:39.107050       1 shared_informer.go:320] Caches are synced for service account
I0513 21:50:39.108951       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0513 21:50:39.109549       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0513 21:50:39.109717       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0513 21:50:39.110023       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 21:50:39.110086       1 shared_informer.go:320] Caches are synced for GC
I0513 21:50:39.110163       1 shared_informer.go:320] Caches are synced for job
I0513 21:50:39.110268       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0513 21:50:39.110702       1 shared_informer.go:320] Caches are synced for HPA
I0513 21:50:39.110778       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0513 21:50:39.113719       1 shared_informer.go:320] Caches are synced for persistent volume
I0513 21:50:39.114464       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0513 21:50:39.123813       1 shared_informer.go:320] Caches are synced for node
I0513 21:50:39.125428       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0513 21:50:39.125905       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0513 21:50:39.126429       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0513 21:50:39.126957       1 shared_informer.go:320] Caches are synced for cidrallocator
I0513 21:50:39.124804       1 shared_informer.go:320] Caches are synced for taint
I0513 21:50:39.129018       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0513 21:50:39.130706       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0513 21:50:39.131279       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0513 21:50:39.423444       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0513 21:50:39.423558       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0513 21:50:40.011510       1 range_allocator.go:433] "Failed to update node PodCIDR after multiple attempts" err="failed to patch node CIDR: Node \"minikube\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.1.0/24\", \"10.244.0.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.1.0/24"]
E0513 21:50:40.011701       1 range_allocator.go:439] "CIDR assignment for node failed. Releasing allocated CIDR" err="failed to patch node CIDR: Node \"minikube\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.1.0/24\", \"10.244.0.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid]" logger="node-ipam-controller" node="minikube"
E0513 21:50:40.011870       1 range_allocator.go:252] "Unhandled Error" err="error syncing 'minikube': failed to patch node CIDR: Node \"minikube\" is invalid: [spec.podCIDRs: Invalid value: []string{\"10.244.1.0/24\", \"10.244.0.0/24\"}: may specify no more than one CIDR for each IP family, spec.podCIDRs: Forbidden: node updates may not change podCIDR except from \"\" to valid], requeuing" logger="UnhandledError"
I0513 21:50:40.012029       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 21:50:40.021903       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 21:50:40.332817       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="699.308231ms"
I0513 21:50:40.504968       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="177.003527ms"
I0513 21:50:40.505148       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="113.316Âµs"
I0513 21:50:40.624345       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="626.589Âµs"
I0513 21:50:40.821210       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="204.729Âµs"
I0513 21:50:41.790116       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="284.808801ms"
I0513 21:50:41.809624       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="19.442958ms"
I0513 21:50:41.809777       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="95.814Âµs"
I0513 21:50:45.188716       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="84.112Âµs"
I0513 21:50:45.229547       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="107.215Âµs"
I0513 21:50:46.958661       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 21:50:59.616246       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="173.932Âµs"
I0513 21:51:00.708216       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="128.223Âµs"
I0513 21:51:00.719208       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="104.419Âµs"
I0513 21:51:08.467483       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="22.210119ms"
I0513 21:51:08.470762       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="140.417Âµs"
I0513 21:53:59.539487       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 21:59:04.956861       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:04:10.277482       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-controller-manager [c5afeb19c228] <==
I0513 22:05:34.712241       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:05:34.719375       1 shared_informer.go:320] Caches are synced for namespace
I0513 22:05:34.722545       1 shared_informer.go:320] Caches are synced for resource quota
I0513 22:05:34.726088       1 shared_informer.go:320] Caches are synced for PV protection
I0513 22:05:34.728324       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0513 22:05:34.728947       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0513 22:05:34.729169       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0513 22:05:34.729589       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0513 22:05:34.730797       1 shared_informer.go:320] Caches are synced for PVC protection
I0513 22:05:34.730823       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0513 22:05:34.733293       1 shared_informer.go:320] Caches are synced for service account
I0513 22:05:34.734585       1 shared_informer.go:320] Caches are synced for job
I0513 22:05:34.735946       1 shared_informer.go:320] Caches are synced for stateful set
I0513 22:05:34.736650       1 shared_informer.go:320] Caches are synced for daemon sets
I0513 22:05:34.745081       1 shared_informer.go:320] Caches are synced for deployment
I0513 22:05:34.746873       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 22:05:34.748102       1 shared_informer.go:320] Caches are synced for crt configmap
I0513 22:05:34.750906       1 shared_informer.go:320] Caches are synced for disruption
I0513 22:05:34.753463       1 shared_informer.go:320] Caches are synced for ReplicationController
I0513 22:05:34.758421       1 shared_informer.go:320] Caches are synced for taint
I0513 22:05:34.759103       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0513 22:05:34.759276       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0513 22:05:34.759332       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0513 22:05:34.760554       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0513 22:05:34.766120       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0513 22:05:34.766149       1 shared_informer.go:320] Caches are synced for persistent volume
I0513 22:05:34.766299       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0513 22:05:34.773755       1 shared_informer.go:320] Caches are synced for attach detach
I0513 22:05:34.773970       1 shared_informer.go:320] Caches are synced for ephemeral
I0513 22:05:34.780182       1 shared_informer.go:320] Caches are synced for GC
I0513 22:05:34.783513       1 shared_informer.go:320] Caches are synced for HPA
I0513 22:05:34.783958       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 22:05:34.784014       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0513 22:05:34.784029       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0513 22:05:34.809018       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0513 22:05:34.809208       1 shared_informer.go:320] Caches are synced for resource quota
I0513 22:05:34.809325       1 shared_informer.go:320] Caches are synced for endpoint
I0513 22:05:34.848789       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="88.001456ms"
I0513 22:05:34.849020       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="94.642Âµs"
I0513 22:05:37.114320       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="48.304062ms"
I0513 22:05:37.114689       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="281.231Âµs"
I0513 22:09:16.653657       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:14:22.407956       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:19:27.788051       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:21:40.509092       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:26:55.968188       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:32:02.675612       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:37:08.428949       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:42:15.414529       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0513 22:43:26.511361       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="168.398672ms"
I0513 22:43:26.541264       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="29.828355ms"
I0513 22:43:26.541528       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="158.819Âµs"
I0513 22:43:26.563948       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="60.907Âµs"
I0513 22:43:30.926517       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="73.409Âµs"
I0513 22:43:34.020210       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="97.011Âµs"
I0513 22:43:44.956668       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="83.609Âµs"
I0513 22:43:46.942253       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="75.709Âµs"
I0513 22:43:59.926560       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="77.709Âµs"
I0513 22:44:05.932844       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="61.907Âµs"
I0513 22:44:12.950018       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/my-app-566d4bf7cf" duration="86.407Âµs"


==> kube-proxy [8f959418a645] <==
I0513 21:50:44.050977       1 server_linux.go:66] "Using iptables proxy"
I0513 21:50:44.412677       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0513 21:50:44.412846       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0513 21:50:44.592646       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0513 21:50:44.592720       1 server_linux.go:170] "Using iptables Proxier"
I0513 21:50:44.609942       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0513 21:50:44.639524       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0513 21:50:44.655254       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0513 21:50:44.655424       1 server.go:497] "Version info" version="v1.32.0"
I0513 21:50:44.655445       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0513 21:50:44.687622       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0513 21:50:44.709806       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0513 21:50:44.712397       1 config.go:105] "Starting endpoint slice config controller"
I0513 21:50:44.712477       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0513 21:50:44.713772       1 config.go:329] "Starting node config controller"
I0513 21:50:44.713810       1 shared_informer.go:313] Waiting for caches to sync for node config
I0513 21:50:44.730780       1 config.go:199] "Starting service config controller"
I0513 21:50:44.742470       1 shared_informer.go:313] Waiting for caches to sync for service config
I0513 21:50:44.742557       1 shared_informer.go:320] Caches are synced for service config
I0513 21:50:44.817341       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0513 21:50:44.818957       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [f797fe73b4dd] <==
I0513 22:05:22.210908       1 server_linux.go:66] "Using iptables proxy"
I0513 22:05:29.649799       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0513 22:05:29.650007       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0513 22:05:30.231030       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0513 22:05:30.231471       1 server_linux.go:170] "Using iptables Proxier"
I0513 22:05:30.241346       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0513 22:05:30.309019       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0513 22:05:30.531578       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0513 22:05:30.531949       1 server.go:497] "Version info" version="v1.32.0"
I0513 22:05:30.532036       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0513 22:05:30.631412       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0513 22:05:30.719592       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0513 22:05:30.725939       1 config.go:199] "Starting service config controller"
I0513 22:05:30.726067       1 shared_informer.go:313] Waiting for caches to sync for service config
I0513 22:05:30.726302       1 config.go:105] "Starting endpoint slice config controller"
I0513 22:05:30.726338       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0513 22:05:30.730636       1 config.go:329] "Starting node config controller"
I0513 22:05:30.730689       1 shared_informer.go:313] Waiting for caches to sync for node config
I0513 22:05:30.829397       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0513 22:05:30.939837       1 shared_informer.go:320] Caches are synced for node config
I0513 22:05:30.948522       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [66f27d360061] <==
I0513 22:05:22.357369       1 serving.go:386] Generated self-signed cert in-memory
W0513 22:05:29.354050       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0513 22:05:29.354105       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0513 22:05:29.354133       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0513 22:05:29.354152       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0513 22:05:29.711066       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0513 22:05:29.714065       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0513 22:05:29.737561       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0513 22:05:29.737730       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0513 22:05:29.738825       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0513 22:05:29.739231       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0513 22:05:29.842534       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [a8c1a150b484] <==
W0513 21:50:29.218031       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0513 21:50:29.225910       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.218068       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0513 21:50:29.225952       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.218165       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:29.225995       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.218431       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0513 21:50:29.226036       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.220909       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0513 21:50:29.226079       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0513 21:50:29.232970       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0513 21:50:29.233054       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.233237       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:29.233291       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.233493       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0513 21:50:29.233543       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.233646       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:29.233691       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.233839       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0513 21:50:29.233877       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:29.233974       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0513 21:50:29.234022       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.067336       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0513 21:50:30.067412       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.076233       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0513 21:50:30.076307       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.103091       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0513 21:50:30.106884       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.110952       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0513 21:50:30.111074       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.219313       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0513 21:50:30.220205       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.266687       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0513 21:50:30.267223       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.340364       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0513 21:50:30.340483       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.342019       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:30.342490       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.357664       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0513 21:50:30.357753       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.376961       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0513 21:50:30.377035       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0513 21:50:30.459303       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:30.459386       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.494505       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:30.494587       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.511711       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:30.511813       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.530494       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0513 21:50:30.530619       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.720864       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0513 21:50:30.720926       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:30.820248       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0513 21:50:30.820358       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0513 21:50:32.442899       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0513 21:50:32.443202       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0513 21:50:38.133138       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0513 22:04:56.707710       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
I0513 22:04:56.707817       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
E0513 22:04:56.708326       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.352247    2513 status_manager.go:890] "Failed to get status for pod" podUID="2b4b75c2a289008e0b381891e9683040" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.353806    2513 status_manager.go:890] "Failed to get status for pod" podUID="d72d0a4cf4be077c9919d46b7358a5e8" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.354474    2513 status_manager.go:890] "Failed to get status for pod" podUID="843c74f7b3bc7d7040a05c31708a6a30" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.409802    2513 status_manager.go:890] "Failed to get status for pod" podUID="fb921fb7-414c-4c52-ab4d-e3212c8aaf12" pod="kube-system/kube-proxy-lh5lp" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-lh5lp\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.553193    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="04cae146d8d863e41a55eb8d1f6df999a19e560c4e1b14cf34aca96a2e20706e"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.554319    2513 status_manager.go:890] "Failed to get status for pod" podUID="2b4b75c2a289008e0b381891e9683040" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.554673    2513 status_manager.go:890] "Failed to get status for pod" podUID="d72d0a4cf4be077c9919d46b7358a5e8" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.555084    2513 status_manager.go:890] "Failed to get status for pod" podUID="843c74f7b3bc7d7040a05c31708a6a30" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.555563    2513 status_manager.go:890] "Failed to get status for pod" podUID="fb921fb7-414c-4c52-ab4d-e3212c8aaf12" pod="kube-system/kube-proxy-lh5lp" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-lh5lp\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.555998    2513 status_manager.go:890] "Failed to get status for pod" podUID="eb3a1c11-4afd-4683-8e5a-912add5c8ed6" pod="kube-system/coredns-668d6bf9bc-5tmrm" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-668d6bf9bc-5tmrm\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.556317    2513 status_manager.go:890] "Failed to get status for pod" podUID="5965174f-b51e-491d-bad5-8743cdce20eb" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:20 minikube kubelet[2513]: I0513 22:05:20.556614    2513 status_manager.go:890] "Failed to get status for pod" podUID="d14ce008bee3a1f3bd7cf547688f9dfe" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.642921    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6dfe781b69cf7e52791e09edb7009bee925b0e430ef480e45904c7a0ec58c80f"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.815312    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="591cc04fffff6741e8d2a406150f4d4233a19419ae021a3022c770bce4a51feb"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.816691    2513 status_manager.go:890] "Failed to get status for pod" podUID="d14ce008bee3a1f3bd7cf547688f9dfe" pod="kube-system/kube-scheduler-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.817975    2513 status_manager.go:890] "Failed to get status for pod" podUID="2b4b75c2a289008e0b381891e9683040" pod="kube-system/etcd-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/etcd-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.818453    2513 status_manager.go:890] "Failed to get status for pod" podUID="d72d0a4cf4be077c9919d46b7358a5e8" pod="kube-system/kube-apiserver-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-apiserver-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.820618    2513 status_manager.go:890] "Failed to get status for pod" podUID="843c74f7b3bc7d7040a05c31708a6a30" pod="kube-system/kube-controller-manager-minikube" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-controller-manager-minikube\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.821244    2513 status_manager.go:890] "Failed to get status for pod" podUID="fb921fb7-414c-4c52-ab4d-e3212c8aaf12" pod="kube-system/kube-proxy-lh5lp" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/kube-proxy-lh5lp\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.821916    2513 status_manager.go:890] "Failed to get status for pod" podUID="eb3a1c11-4afd-4683-8e5a-912add5c8ed6" pod="kube-system/coredns-668d6bf9bc-5tmrm" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/coredns-668d6bf9bc-5tmrm\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:21 minikube kubelet[2513]: I0513 22:05:21.822611    2513 status_manager.go:890] "Failed to get status for pod" podUID="5965174f-b51e-491d-bad5-8743cdce20eb" pod="kube-system/storage-provisioner" err="Get \"https://control-plane.minikube.internal:8443/api/v1/namespaces/kube-system/pods/storage-provisioner\": dial tcp 192.168.49.2:8443: connect: connection refused"
May 13 22:05:22 minikube kubelet[2513]: I0513 22:05:22.114477    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="160df0daee173ef7d9b70726e4fc0a684431c0d52137bad095e38c94db8bf759"
May 13 22:05:23 minikube kubelet[2513]: I0513 22:05:23.718235    2513 scope.go:117] "RemoveContainer" containerID="329e1d9e3ba97a4556d622b55ef0a66b8d77ce7adb791814b4362288cba03ed2"
May 13 22:05:23 minikube kubelet[2513]: I0513 22:05:23.732326    2513 scope.go:117] "RemoveContainer" containerID="f71bd37628f190d02f6d5a77866cf8a06161f2c2a2af2936dca0f2fc315282b6"
May 13 22:05:23 minikube kubelet[2513]: E0513 22:05:23.732703    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(5965174f-b51e-491d-bad5-8743cdce20eb)\"" pod="kube-system/storage-provisioner" podUID="5965174f-b51e-491d-bad5-8743cdce20eb"
May 13 22:05:29 minikube kubelet[2513]: I0513 22:05:29.515178    2513 status_manager.go:890] "Failed to get status for pod" podUID="eb3a1c11-4afd-4683-8e5a-912add5c8ed6" pod="kube-system/coredns-668d6bf9bc-5tmrm" err="pods \"coredns-668d6bf9bc-5tmrm\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object"
May 13 22:05:29 minikube kubelet[2513]: E0513 22:05:29.517817    2513 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"coredns\": Failed to watch *v1.ConfigMap: configmaps \"coredns\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
May 13 22:05:29 minikube kubelet[2513]: E0513 22:05:29.518526    2513 reflector.go:166] "Unhandled Error" err="object-\"kube-system\"/\"kube-root-ca.crt\": Failed to watch *v1.ConfigMap: configmaps \"kube-root-ca.crt\" is forbidden: User \"system:node:minikube\" cannot watch resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError"
May 13 22:05:35 minikube kubelet[2513]: I0513 22:05:35.244678    2513 scope.go:117] "RemoveContainer" containerID="f71bd37628f190d02f6d5a77866cf8a06161f2c2a2af2936dca0f2fc315282b6"
May 13 22:19:55 minikube kubelet[2513]: E0513 22:19:55.522606    2513 controller.go:195] "Failed to update lease" err="etcdserver: request timed out"
May 13 22:19:56 minikube kubelet[2513]: E0513 22:19:56.811819    2513 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"minikube\": the object has been modified; please apply your changes to the latest version and try again"
May 13 22:43:26 minikube kubelet[2513]: I0513 22:43:26.398868    2513 memory_manager.go:355] "RemoveStaleState removing state" podUID="eb3941f3-e612-4f32-8dae-b84d0b0c9a70" containerName="coredns"
May 13 22:43:26 minikube kubelet[2513]: I0513 22:43:26.520884    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v9dcp\" (UniqueName: \"kubernetes.io/projected/b72c896e-a2e5-4489-9536-7a7d7691e66f-kube-api-access-v9dcp\") pod \"my-app-566d4bf7cf-mwsgh\" (UID: \"b72c896e-a2e5-4489-9536-7a7d7691e66f\") " pod="default/my-app-566d4bf7cf-mwsgh"
May 13 22:43:26 minikube kubelet[2513]: I0513 22:43:26.622725    2513 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-bl8cr\" (UniqueName: \"kubernetes.io/projected/53066d95-9289-45ea-8c86-1c7287e3f068-kube-api-access-bl8cr\") pod \"my-app-566d4bf7cf-x4fpx\" (UID: \"53066d95-9289-45ea-8c86-1c7287e3f068\") " pod="default/my-app-566d4bf7cf-x4fpx"
May 13 22:43:27 minikube kubelet[2513]: I0513 22:43:27.666205    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="b751ab92471ad87ac38ec78d7149d55a72625b2973041655a4efc2fe5fde37ac"
May 13 22:43:27 minikube kubelet[2513]: I0513 22:43:27.793398    2513 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e1d98cabd58d88fd274f8e26840cf40150b7470430c764844e989525caaeaeef"
May 13 22:43:30 minikube kubelet[2513]: E0513 22:43:30.737465    2513 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:30 minikube kubelet[2513]: E0513 22:43:30.737643    2513 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:30 minikube kubelet[2513]: E0513 22:43:30.738234    2513 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-app,Image:rkaghzi/my-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9dcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-app-566d4bf7cf-mwsgh_default(b72c896e-a2e5-4489-9536-7a7d7691e66f): ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 13 22:43:30 minikube kubelet[2513]: E0513 22:43:30.744956    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-mwsgh" podUID="b72c896e-a2e5-4489-9536-7a7d7691e66f"
May 13 22:43:30 minikube kubelet[2513]: E0513 22:43:30.892927    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rkaghzi/my-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-mwsgh" podUID="b72c896e-a2e5-4489-9536-7a7d7691e66f"
May 13 22:43:33 minikube kubelet[2513]: E0513 22:43:33.780101    2513 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:33 minikube kubelet[2513]: E0513 22:43:33.780269    2513 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:33 minikube kubelet[2513]: E0513 22:43:33.780549    2513 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-app,Image:rkaghzi/my-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl8cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-app-566d4bf7cf-x4fpx_default(53066d95-9289-45ea-8c86-1c7287e3f068): ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 13 22:43:33 minikube kubelet[2513]: E0513 22:43:33.782032    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-x4fpx" podUID="53066d95-9289-45ea-8c86-1c7287e3f068"
May 13 22:43:33 minikube kubelet[2513]: E0513 22:43:33.963533    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rkaghzi/my-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-x4fpx" podUID="53066d95-9289-45ea-8c86-1c7287e3f068"
May 13 22:43:47 minikube kubelet[2513]: E0513 22:43:47.838577    2513 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:47 minikube kubelet[2513]: E0513 22:43:47.838889    2513 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:47 minikube kubelet[2513]: E0513 22:43:47.839464    2513 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-app,Image:rkaghzi/my-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9dcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-app-566d4bf7cf-mwsgh_default(b72c896e-a2e5-4489-9536-7a7d7691e66f): ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 13 22:43:47 minikube kubelet[2513]: E0513 22:43:47.840874    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-mwsgh" podUID="b72c896e-a2e5-4489-9536-7a7d7691e66f"
May 13 22:43:50 minikube kubelet[2513]: E0513 22:43:50.771555    2513 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:50 minikube kubelet[2513]: E0513 22:43:50.771871    2513 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:43:50 minikube kubelet[2513]: E0513 22:43:50.772303    2513 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-app,Image:rkaghzi/my-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bl8cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-app-566d4bf7cf-x4fpx_default(53066d95-9289-45ea-8c86-1c7287e3f068): ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 13 22:43:50 minikube kubelet[2513]: E0513 22:43:50.774633    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-x4fpx" podUID="53066d95-9289-45ea-8c86-1c7287e3f068"
May 13 22:43:59 minikube kubelet[2513]: E0513 22:43:59.908674    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rkaghzi/my-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-mwsgh" podUID="b72c896e-a2e5-4489-9536-7a7d7691e66f"
May 13 22:44:05 minikube kubelet[2513]: E0513 22:44:05.909176    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ImagePullBackOff: \"Back-off pulling image \\\"rkaghzi/my-app:latest\\\": ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-x4fpx" podUID="53066d95-9289-45ea-8c86-1c7287e3f068"
May 13 22:44:15 minikube kubelet[2513]: E0513 22:44:15.719289    2513 log.go:32] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:44:15 minikube kubelet[2513]: E0513 22:44:15.719451    2513 kuberuntime_image.go:55] "Failed to pull image" err="Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="rkaghzi/my-app:latest"
May 13 22:44:15 minikube kubelet[2513]: E0513 22:44:15.719670    2513 kuberuntime_manager.go:1341] "Unhandled Error" err="container &Container{Name:my-app,Image:rkaghzi/my-app:latest,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:3000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9dcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod my-app-566d4bf7cf-mwsgh_default(b72c896e-a2e5-4489-9536-7a7d7691e66f): ErrImagePull: Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" logger="UnhandledError"
May 13 22:44:15 minikube kubelet[2513]: E0513 22:44:15.721069    2513 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"my-app\" with ErrImagePull: \"Error response from daemon: pull access denied for rkaghzi/my-app, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="default/my-app-566d4bf7cf-mwsgh" podUID="b72c896e-a2e5-4489-9536-7a7d7691e66f"


==> storage-provisioner [0012aa195056] <==
I0513 22:05:35.632820       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0513 22:05:35.649142       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0513 22:05:35.649212       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0513 22:05:53.129633       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0513 22:05:53.130145       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_26ca3274-45e2-432d-91af-98e09652ed5e!
I0513 22:05:53.131050       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"3c09e4c8-f7a4-4755-9c63-829bd4e6361b", APIVersion:"v1", ResourceVersion:"1268", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_26ca3274-45e2-432d-91af-98e09652ed5e became leader
I0513 22:05:53.231531       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_26ca3274-45e2-432d-91af-98e09652ed5e!


==> storage-provisioner [f71bd37628f1] <==
I0513 22:05:20.329223       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0513 22:05:20.760791       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

